                      
"""
åˆ†å±‚ç­‰ä»·æ€§æ£€æŸ¥ç³»ç»Ÿ
Level 1: æ§åˆ¶æµç­‰ä»·æ€§
Level 2: å†…å­˜è®¿é—®æ¨¡å¼ç­‰ä»·æ€§  
Level 3: æ•°æ®å˜æ¢ç­‰ä»·æ€§

è§£å†³ä¼ ç»Ÿç¬¦å·æ‰§è¡Œçº¦æŸè¡¨ç¤ºå±‚æ¬¡è¿‡é«˜çš„é—®é¢˜
"""

import re
import z3
from z3 import *
import glob
import time
import datetime
import os
from collections import defaultdict, Counter
from dataclasses import dataclass
from typing import List, Dict, Set, Tuple, Optional
import hashlib

@dataclass
class ConstraintAnalysis:
    """çº¦æŸåˆ†æç»“æœ"""
    control_flow_constraints: List[str]
    memory_access_constraints: List[str]
    data_transformation_constraints: List[str]
    variable_bounds: Dict[str, Tuple[int, int]]
    memory_addresses: Set[str]
    arithmetic_operations: List[str]

@dataclass 
class LayeredEquivalenceResult:
    """åˆ†å±‚ç­‰ä»·æ€§æ£€æŸ¥ç»“æœ"""
    level1_control_flow: str                                       
    level2_memory_access: str
    level3_data_transformation: str
    overall_result: str
    confidence_score: float
    detailed_analysis: Dict
    
class ConstraintClassifier:
    """çº¦æŸåˆ†ç±»å™¨ - å°†SMTçº¦æŸåˆ†ç±»åˆ°ä¸åŒå±‚æ¬¡"""
    
    def __init__(self):
               
        self.control_flow_patterns = [
            r'bvslt.*scanf_0',          
            r'bvsge.*scanf_0',            
            r'bvuge.*scanf_0',          
            r'bvule.*scanf_0',          
            r'distinct.*scanf_0',       
        ]
        
                
        self.memory_access_patterns = [
            r'distinct.*bv\d+.*64',           
            r'bvule.*bv\d{7,}',              
            r'bvadd.*bv\d{7,}',            
            r'bvshl.*bv\d+.*64',             
        ]
        
                  
        self.data_transformation_patterns = [
            r'bvadd.*extract',          
            r'bvsub.*extract',          
            r'bvmul.*extract',          
            r'bvand.*extract',       
            r'bvor.*extract',        
            r'select.*store',           
        ]
    
    def classify_constraint(self, constraint: str) -> str:
        """å°†çº¦æŸåˆ†ç±»åˆ°æ§åˆ¶æµã€å†…å­˜è®¿é—®æˆ–æ•°æ®å˜æ¢"""
                    
        clean_constraint = re.sub(r'\s+', ' ', constraint.strip())
        
                               
        for pattern in self.data_transformation_patterns:
            if re.search(pattern, clean_constraint, re.IGNORECASE):
                return 'data_transformation'
        
        for pattern in self.memory_access_patterns:
            if re.search(pattern, clean_constraint, re.IGNORECASE):
                return 'memory_access'
                
        for pattern in self.control_flow_patterns:
            if re.search(pattern, clean_constraint, re.IGNORECASE):
                return 'control_flow'
        
                  
        return 'control_flow'
    
    def extract_memory_addresses(self, constraints: List[str]) -> Set[str]:
        """æå–çº¦æŸä¸­çš„å†…å­˜åœ°å€"""
        addresses = set()
        for constraint in constraints:
                                         
            addr_matches = re.findall(r'_\s+bv(\d{7,})\s+64', constraint)
            addresses.update(addr_matches)
        return addresses
    
    def extract_arithmetic_operations(self, constraints: List[str]) -> List[str]:
        """æå–ç®—æœ¯è¿ç®—æ“ä½œ"""
        operations = []
        for constraint in constraints:
                    
            ops = re.findall(r'(bvadd|bvsub|bvmul|bvdiv|bvand|bvor|bvxor)', constraint)
            operations.extend(ops)
        return operations
    
    def analyze_constraints(self, constraints: List[str], variables: Dict[str, int]) -> ConstraintAnalysis:
        """å…¨é¢åˆ†æçº¦æŸç»“æ„"""
        control_flow = []
        memory_access = []
        data_transformation = []
        
        for constraint in constraints:
            category = self.classify_constraint(constraint)
            if category == 'control_flow':
                control_flow.append(constraint)
            elif category == 'memory_access':
                memory_access.append(constraint)
            else:
                data_transformation.append(constraint)
        
                
        variable_bounds = {}
        for var_name in variables:
            bounds = self.extract_variable_bounds(constraints, var_name)
            if bounds:
                variable_bounds[var_name] = bounds
        
        return ConstraintAnalysis(
            control_flow_constraints=control_flow,
            memory_access_constraints=memory_access,
            data_transformation_constraints=data_transformation,
            variable_bounds=variable_bounds,
            memory_addresses=self.extract_memory_addresses(constraints),
            arithmetic_operations=self.extract_arithmetic_operations(constraints)
        )
    
    def extract_variable_bounds(self, constraints: List[str], var_name: str) -> Optional[Tuple[int, int]]:
        """æå–å˜é‡çš„ä¸Šä¸‹ç•Œ"""
        lower_bound = None
        upper_bound = None
        
        for constraint in constraints:
                                       
            lower_match = re.search(rf'bvuge\s+{var_name}.*bv(\d+)\s+32', constraint)
            if lower_match:
                lower_bound = int(lower_match.group(1))
            
                                         
            upper_match = re.search(rf'bvule\s+{var_name}.*bv(\d+)\s+32', constraint)
            if upper_match:
                upper_bound = int(upper_match.group(1))
        
        if lower_bound is not None and upper_bound is not None:
            return (lower_bound, upper_bound)
        return None

class LayeredEquivalenceChecker:
    """åˆ†å±‚ç­‰ä»·æ€§æ£€æŸ¥å™¨"""
    
    def __init__(self, timeout=30000):
        self.timeout = timeout
        self.classifier = ConstraintClassifier()
        self.z3_total_time = 0.0
        self.z3_call_count = 0
    
    def extract_constraint_formula(self, file_path: str):
        """ä»æ–‡ä»¶ä¸­æå–çº¦æŸå…¬å¼ï¼ˆå¤ç”¨åŸæœ‰æ–¹æ³•ï¼‰"""
        with open(file_path, 'r') as f:
            content = f.read()
        
              
        lines = [line for line in content.splitlines() if not line.strip().startswith(';')]
        content = '\n'.join(lines)
        
                
        variables = {}
        var_pattern = r'\(declare-fun\s+(\w+)\s+\(\)\s+\(_\s+BitVec\s+(\d+)\)\)'
        for match in re.finditer(var_pattern, content):
            var_name, bit_width = match.groups()
            variables[var_name] = int(bit_width)
        
              
        constraints = []
        constraint_pattern = r'\(assert\s+(.*?)\)(?=\s*(?:\(assert|\(check-sat|$))'
        for match in re.finditer(constraint_pattern, content, re.DOTALL):
            constraint = match.group(1).strip()
            constraints.append(constraint)
        
        return variables, constraints
    
    def check_level1_control_flow_equivalence(self, analysis1: ConstraintAnalysis, 
                                            analysis2: ConstraintAnalysis) -> Tuple[str, Dict]:
        """Level 1: æ§åˆ¶æµç­‰ä»·æ€§æ£€æŸ¥"""
        print("    ğŸ“Š Level 1: æ§åˆ¶æµç­‰ä»·æ€§æ£€æŸ¥")
        
        start_time = time.time()
        
                      
        cf1 = analysis1.control_flow_constraints
        cf2 = analysis2.control_flow_constraints
        
        details = {
            'control_flow_count1': len(cf1),
            'control_flow_count2': len(cf2),
            'variable_bounds1': analysis1.variable_bounds,
            'variable_bounds2': analysis2.variable_bounds,
        }
        
                
        bounds_equivalent = True
        if analysis1.variable_bounds != analysis2.variable_bounds:
            bounds_equivalent = False
            details['bounds_difference'] = {
                'path1_bounds': analysis1.variable_bounds,
                'path2_bounds': analysis2.variable_bounds
            }
        
                     
        count_diff = abs(len(cf1) - len(cf2))
        if count_diff > 2:          
            result = "not_equivalent"
            details['reason'] = f"æ§åˆ¶æµçº¦æŸæ•°é‡å·®å¼‚è¿‡å¤§: {len(cf1)} vs {len(cf2)}"
        elif not bounds_equivalent:
            result = "not_equivalent" 
            details['reason'] = "å˜é‡è¾¹ç•Œä¸ä¸€è‡´"
        else:
            result = "equivalent"
            details['reason'] = "æ§åˆ¶æµç»“æ„åŸºæœ¬ä¸€è‡´"
        
        solve_time = time.time() - start_time
        details['check_time'] = solve_time
        
        print(f"      ç»“æœ: {result}")
        print(f"      æ§åˆ¶æµçº¦æŸ: {len(cf1)} vs {len(cf2)}")
        print(f"      å˜é‡è¾¹ç•Œ: {analysis1.variable_bounds} vs {analysis2.variable_bounds}")
        
        return result, details
    
    def check_level2_memory_access_equivalence(self, analysis1: ConstraintAnalysis,
                                             analysis2: ConstraintAnalysis) -> Tuple[str, Dict]:
        """Level 2: å†…å­˜è®¿é—®æ¨¡å¼ç­‰ä»·æ€§æ£€æŸ¥"""
        print("    ğŸ” Level 2: å†…å­˜è®¿é—®æ¨¡å¼ç­‰ä»·æ€§æ£€æŸ¥")
        
        start_time = time.time()
        
        ma1 = analysis1.memory_access_constraints
        ma2 = analysis2.memory_access_constraints
        
        details = {
            'memory_access_count1': len(ma1),
            'memory_access_count2': len(ma2),
            'memory_addresses1': analysis1.memory_addresses,
            'memory_addresses2': analysis2.memory_addresses,
        }
        
                  
        addr_intersection = analysis1.memory_addresses & analysis2.memory_addresses
        addr_union = analysis1.memory_addresses | analysis2.memory_addresses
        
        if len(addr_union) == 0:
            addr_similarity = 1.0           
        else:
            addr_similarity = len(addr_intersection) / len(addr_union)
        
        details['address_similarity'] = addr_similarity
        details['common_addresses'] = list(addr_intersection)
        details['unique_addresses1'] = list(analysis1.memory_addresses - analysis2.memory_addresses)
        details['unique_addresses2'] = list(analysis2.memory_addresses - analysis1.memory_addresses)
        
                  
        access_count_diff = abs(len(ma1) - len(ma2))
        
        if access_count_diff > 5 and addr_similarity < 0.3:
            result = "not_equivalent"
            details['reason'] = f"å†…å­˜è®¿é—®æ¨¡å¼æ˜¾è‘—ä¸åŒ: çº¦æŸæ•°å·®å¼‚{access_count_diff}, åœ°å€ç›¸ä¼¼åº¦{addr_similarity:.2f}"
        elif addr_similarity < 0.1:
            result = "not_equivalent"
            details['reason'] = f"å†…å­˜åœ°å€é›†åˆå‡ ä¹å®Œå…¨ä¸åŒ: ç›¸ä¼¼åº¦{addr_similarity:.2f}"
        elif addr_similarity > 0.8 and access_count_diff <= 3:
            result = "equivalent"
            details['reason'] = f"å†…å­˜è®¿é—®æ¨¡å¼é«˜åº¦ç›¸ä¼¼: ç›¸ä¼¼åº¦{addr_similarity:.2f}"
        else:
            result = "partial_equivalent"
            details['reason'] = f"å†…å­˜è®¿é—®æ¨¡å¼éƒ¨åˆ†ç›¸ä¼¼: ç›¸ä¼¼åº¦{addr_similarity:.2f}"
        
        solve_time = time.time() - start_time
        details['check_time'] = solve_time
        
        print(f"      ç»“æœ: {result}")
        print(f"      å†…å­˜è®¿é—®çº¦æŸ: {len(ma1)} vs {len(ma2)}")
        print(f"      åœ°å€ç›¸ä¼¼åº¦: {addr_similarity:.2f}")
        print(f"      å…±åŒåœ°å€: {len(addr_intersection)} ä¸ª")
        
        return result, details
    
    def check_level3_data_transformation_equivalence(self, analysis1: ConstraintAnalysis,
                                                   analysis2: ConstraintAnalysis) -> Tuple[str, Dict]:
        """Level 3: æ•°æ®å˜æ¢ç­‰ä»·æ€§æ£€æŸ¥"""
        print("    ğŸ§® Level 3: æ•°æ®å˜æ¢ç­‰ä»·æ€§æ£€æŸ¥")
        
        start_time = time.time()
        
        dt1 = analysis1.data_transformation_constraints
        dt2 = analysis2.data_transformation_constraints
        
                  
        ops1 = Counter(analysis1.arithmetic_operations)
        ops2 = Counter(analysis2.arithmetic_operations)
        
        details = {
            'data_transformation_count1': len(dt1),
            'data_transformation_count2': len(dt2),
            'arithmetic_operations1': dict(ops1),
            'arithmetic_operations2': dict(ops2),
        }
        
                   
        all_ops = set(ops1.keys()) | set(ops2.keys())
        if len(all_ops) == 0:
            op_similarity = 1.0           
        else:
            similarity_sum = 0
            for op in all_ops:
                count1 = ops1.get(op, 0)
                count2 = ops2.get(op, 0)
                max_count = max(count1, count2)
                if max_count > 0:
                    similarity_sum += min(count1, count2) / max_count
            op_similarity = similarity_sum / len(all_ops)
        
        details['operation_similarity'] = op_similarity
        
                   
        complexity_diff = abs(len(dt1) - len(dt2))
        
        if len(dt1) == 0 and len(dt2) == 0:
            result = "equivalent"
            details['reason'] = "éƒ½æ²¡æœ‰æ•°æ®å˜æ¢æ“ä½œ"
        elif op_similarity > 0.8 and complexity_diff <= 2:
            result = "equivalent"
            details['reason'] = f"æ•°æ®å˜æ¢æ¨¡å¼é«˜åº¦ç›¸ä¼¼: è¿ç®—ç›¸ä¼¼åº¦{op_similarity:.2f}"
        elif op_similarity < 0.3:
            result = "not_equivalent"
            details['reason'] = f"æ•°æ®å˜æ¢æ¨¡å¼æ˜¾è‘—ä¸åŒ: è¿ç®—ç›¸ä¼¼åº¦{op_similarity:.2f}"
        else:
            result = "partial_equivalent"
            details['reason'] = f"æ•°æ®å˜æ¢æ¨¡å¼éƒ¨åˆ†ç›¸ä¼¼: è¿ç®—ç›¸ä¼¼åº¦{op_similarity:.2f}"
        
        solve_time = time.time() - start_time
        details['check_time'] = solve_time
        
        print(f"      ç»“æœ: {result}")
        print(f"      æ•°æ®å˜æ¢çº¦æŸ: {len(dt1)} vs {len(dt2)}")
        print(f"      è¿ç®—æ¨¡å¼ç›¸ä¼¼åº¦: {op_similarity:.2f}")
        print(f"      ç®—æœ¯æ“ä½œ: {dict(ops1)} vs {dict(ops2)}")
        
        return result, details
    
    def calculate_confidence_score(self, level1_result: str, level2_result: str, 
                                 level3_result: str) -> float:
        """è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°"""
        score_map = {
            'equivalent': 1.0,
            'partial_equivalent': 0.5,
            'not_equivalent': 0.0,
            'unknown': 0.3
        }
        
        scores = [
            score_map.get(level1_result, 0.3) * 0.3,            
            score_map.get(level2_result, 0.3) * 0.4,             
            score_map.get(level3_result, 0.3) * 0.3              
        ]
        
        return sum(scores)
    
    def determine_overall_result(self, level1_result: str, level2_result: str, 
                               level3_result: str, confidence: float) -> str:
        """ç¡®å®šæ€»ä½“ç­‰ä»·æ€§ç»“æœ"""
                                   
        if 'not_equivalent' in [level1_result, level2_result, level3_result]:
            if confidence < 0.4:
                return 'not_equivalent'
            elif confidence < 0.7:
                return 'likely_not_equivalent'
        
                  
        if all(result == 'equivalent' for result in [level1_result, level2_result, level3_result]):
            return 'equivalent'
        
                  
        equiv_count = sum(1 for result in [level1_result, level2_result, level3_result] 
                         if result == 'equivalent')
        
        if equiv_count >= 2:
            return 'likely_equivalent'
        elif confidence > 0.7:
            return 'partial_equivalent'
        else:
            return 'not_equivalent'
    
    def check_layered_equivalence(self, file1: str, file2: str) -> LayeredEquivalenceResult:
        """æ‰§è¡Œåˆ†å±‚ç­‰ä»·æ€§æ£€æŸ¥"""
        print(f"\nğŸ”¬ åˆ†å±‚ç­‰ä»·æ€§æ£€æŸ¥: {os.path.basename(file1)} vs {os.path.basename(file2)}")
        
              
        try:
            vars1, constraints1 = self.extract_constraint_formula(file1)
            vars2, constraints2 = self.extract_constraint_formula(file2)
        except Exception as e:
            return LayeredEquivalenceResult(
                level1_control_flow="error",
                level2_memory_access="error", 
                level3_data_transformation="error",
                overall_result="error",
                confidence_score=0.0,
                detailed_analysis={"error": str(e)}
            )
        
                
        analysis1 = self.classifier.analyze_constraints(constraints1, vars1)
        analysis2 = self.classifier.analyze_constraints(constraints2, vars2)
        
        print(f"  è·¯å¾„1: {len(constraints1)} çº¦æŸ -> CF:{len(analysis1.control_flow_constraints)} MA:{len(analysis1.memory_access_constraints)} DT:{len(analysis1.data_transformation_constraints)}")
        print(f"  è·¯å¾„2: {len(constraints2)} çº¦æŸ -> CF:{len(analysis2.control_flow_constraints)} MA:{len(analysis2.memory_access_constraints)} DT:{len(analysis2.data_transformation_constraints)}")
        
              
        level1_result, level1_details = self.check_level1_control_flow_equivalence(analysis1, analysis2)
        level2_result, level2_details = self.check_level2_memory_access_equivalence(analysis1, analysis2)
        level3_result, level3_details = self.check_level3_data_transformation_equivalence(analysis1, analysis2)
        
                    
        confidence = self.calculate_confidence_score(level1_result, level2_result, level3_result)
        overall_result = self.determine_overall_result(level1_result, level2_result, level3_result, confidence)
        
        print(f"\n  ğŸ“Š åˆ†å±‚ç»“æœ:")
        print(f"    Level 1 (æ§åˆ¶æµ): {level1_result}")
        print(f"    Level 2 (å†…å­˜è®¿é—®): {level2_result}")
        print(f"    Level 3 (æ•°æ®å˜æ¢): {level3_result}")
        print(f"    æ•´ä½“ç»“æœ: {overall_result}")
        print(f"    ç½®ä¿¡åº¦: {confidence:.2f}")
        
        return LayeredEquivalenceResult(
            level1_control_flow=level1_result,
            level2_memory_access=level2_result,
            level3_data_transformation=level3_result,
            overall_result=overall_result,
            confidence_score=confidence,
            detailed_analysis={
                'level1_details': level1_details,
                'level2_details': level2_details,
                'level3_details': level3_details,
                'analysis1': analysis1,
                'analysis2': analysis2
            }
        )

class EnhancedEquivalenceAnalyzer:
    """å¢å¼ºç­‰ä»·æ€§åˆ†æå™¨"""
    
    def __init__(self, benchmark_dir: str = '.'):
        self.benchmark_dir = benchmark_dir
        self.checker = LayeredEquivalenceChecker()
        
    def analyze_path_pair(self, file1: str, file2: str) -> LayeredEquivalenceResult:
        """åˆ†æå•ä¸ªè·¯å¾„å¯¹"""
        return self.checker.check_layered_equivalence(file1, file2)
    
    def generate_layered_report(self, results: List[Tuple[str, str, LayeredEquivalenceResult]], 
                               output_file: str = "layered_equivalence_report.txt"):
        """ç”Ÿæˆåˆ†å±‚åˆ†ææŠ¥å‘Š"""
        with open(output_file, "w", encoding='utf-8') as f:
            f.write("åˆ†å±‚ç­‰ä»·æ€§åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            
            f.write("åˆ†ææ–¹æ³•:\n")
            f.write("  Level 1: æ§åˆ¶æµç­‰ä»·æ€§ (å¾ªç¯è¾¹ç•Œã€åˆ†æ”¯æ¡ä»¶)\n")
            f.write("  Level 2: å†…å­˜è®¿é—®æ¨¡å¼ç­‰ä»·æ€§ (åœ°å€æ¨¡å¼ã€è®¿é—®é¢‘ç‡)\n") 
            f.write("  Level 3: æ•°æ®å˜æ¢ç­‰ä»·æ€§ (ç®—æœ¯è¿ç®—ã€æ•°æ®æµ)\n\n")
            
                  
            total_pairs = len(results)
            overall_equivalent = sum(1 for _, _, result in results if result.overall_result == 'equivalent')
            overall_not_equivalent = sum(1 for _, _, result in results if result.overall_result == 'not_equivalent')
            
            f.write(f"æ€»ä½“ç»Ÿè®¡:\n")
            f.write(f"  åˆ†æè·¯å¾„å¯¹æ•°: {total_pairs}\n")
            f.write(f"  æ•´ä½“ç­‰ä»·: {overall_equivalent} ({overall_equivalent/total_pairs*100:.1f}%)\n")
            f.write(f"  æ•´ä½“ä¸ç­‰ä»·: {overall_not_equivalent} ({overall_not_equivalent/total_pairs*100:.1f}%)\n")
            f.write(f"  å…¶ä»–æƒ…å†µ: {total_pairs - overall_equivalent - overall_not_equivalent}\n\n")
            
                  
            level1_eq = sum(1 for _, _, result in results if result.level1_control_flow == 'equivalent')
            level2_eq = sum(1 for _, _, result in results if result.level2_memory_access == 'equivalent') 
            level3_eq = sum(1 for _, _, result in results if result.level3_data_transformation == 'equivalent')
            
            f.write(f"åˆ†å±‚ç­‰ä»·æ€§ç»Ÿè®¡:\n")
            f.write(f"  Level 1 (æ§åˆ¶æµ) ç­‰ä»·: {level1_eq}/{total_pairs} ({level1_eq/total_pairs*100:.1f}%)\n")
            f.write(f"  Level 2 (å†…å­˜è®¿é—®) ç­‰ä»·: {level2_eq}/{total_pairs} ({level2_eq/total_pairs*100:.1f}%)\n")
            f.write(f"  Level 3 (æ•°æ®å˜æ¢) ç­‰ä»·: {level3_eq}/{total_pairs} ({level3_eq/total_pairs*100:.1f}%)\n\n")
            
                  
            f.write("è¯¦ç»†åˆ†æç»“æœ:\n")
            f.write("-" * 60 + "\n")
            
            for file1, file2, result in results:
                f.write(f"\næ¯”è¾ƒ: {os.path.basename(file1)} vs {os.path.basename(file2)}\n")
                f.write(f"  Level 1 (æ§åˆ¶æµ): {result.level1_control_flow}\n")
                f.write(f"  Level 2 (å†…å­˜è®¿é—®): {result.level2_memory_access}\n")
                f.write(f"  Level 3 (æ•°æ®å˜æ¢): {result.level3_data_transformation}\n")
                f.write(f"  æ•´ä½“ç»“æœ: {result.overall_result}\n")
                f.write(f"  ç½®ä¿¡åº¦: {result.confidence_score:.2f}\n")
                
                      
                if 'level1_details' in result.detailed_analysis:
                    level1 = result.detailed_analysis['level1_details']
                    f.write(f"  æ§åˆ¶æµè¯¦æƒ…: {level1.get('reason', '')}\n")
                
                if 'level2_details' in result.detailed_analysis:
                    level2 = result.detailed_analysis['level2_details']
                    f.write(f"  å†…å­˜è®¿é—®è¯¦æƒ…: {level2.get('reason', '')}\n")
                    if 'address_similarity' in level2:
                        f.write(f"    åœ°å€ç›¸ä¼¼åº¦: {level2['address_similarity']:.2f}\n")
                
                if 'level3_details' in result.detailed_analysis:
                    level3 = result.detailed_analysis['level3_details']
                    f.write(f"  æ•°æ®å˜æ¢è¯¦æƒ…: {level3.get('reason', '')}\n")
                    if 'operation_similarity' in level3:
                        f.write(f"    è¿ç®—ç›¸ä¼¼åº¦: {level3['operation_similarity']:.2f}\n")
        
        print(f"ğŸ“„ åˆ†å±‚åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='åˆ†å±‚ç­‰ä»·æ€§æ£€æŸ¥ç³»ç»Ÿ')
    parser.add_argument('--file1', help='ç¬¬ä¸€ä¸ªè·¯å¾„æ–‡ä»¶')
    parser.add_argument('--file2', help='ç¬¬äºŒä¸ªè·¯å¾„æ–‡ä»¶')
    parser.add_argument('--benchmark', default='.', help='åŸºå‡†æµ‹è¯•ç›®å½•')
    parser.add_argument('--output', default='layered_equivalence_report.txt', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶')
    
    args = parser.parse_args()
    
    analyzer = EnhancedEquivalenceAnalyzer(args.benchmark)
    
    if args.file1 and args.file2:
              
        result = analyzer.analyze_path_pair(args.file1, args.file2)
        results = [(args.file1, args.file2, result)]
        analyzer.generate_layered_report(results, args.output)
    else:
        print("è¯·æä¾›è¦æ¯”è¾ƒçš„ä¸¤ä¸ªè·¯å¾„æ–‡ä»¶: --file1 <file1> --file2 <file2>")

if __name__ == "__main__":
    main() 