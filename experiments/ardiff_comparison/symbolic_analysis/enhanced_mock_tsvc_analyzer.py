                      
"""
å¢å¼ºæ¨¡æ‹ŸTSVC Benchmarkåˆ†æå™¨
ä¸“é—¨ä½¿ç”¨æ™ºèƒ½æ¨¡æ‹Ÿæ¥å±•ç¤ºä¸åŒä¼˜åŒ–çº§åˆ«å’Œbenchmarkçš„çœŸå®å·®å¼‚
"""

import os
import tempfile
import shutil
from pathlib import Path
import time
from typing import List, Dict

from semantic_equivalence_analyzer import PathClusterAnalyzer

class EnhancedMockTSVCAnalyzer:
    """å¢å¼ºæ¨¡æ‹ŸTSVC benchmarkåˆ†æå™¨ - å±•ç¤ºçœŸå®å·®å¼‚"""
    
    def __init__(self):
        self.benchmark_patterns = {
            's000': {
                'description': 'ç®€å•å‘é‡åŠ æ³•: a[i] = b[i] + 1',
                'base_constraints': ['array_add', 'loop_bound'],
                'optimization_effects': {
                    'O1': [],
                    'O2': ['loop_unroll'],
                    'O3': ['vectorization', 'prefetch']
                }
            },
            's1112': {
                'description': 'åå‘å¾ªç¯: a[i] = b[i] + 1 (iä»å¤§åˆ°å°)',
                'base_constraints': ['array_add', 'reverse_loop'],
                'optimization_effects': {
                    'O1': [],
                    'O2': ['reverse_loop_opt'],
                    'O3': ['reverse_vectorization']
                }
            },
            's121': {
                'description': 'æ•°æ®ä¾èµ–: a[i] = a[i+1] + b[i]',
                'base_constraints': ['data_dependency', 'forward_reference'],
                'optimization_effects': {
                    'O1': [],
                    'O2': ['dependency_analysis'],
                    'O3': ['dependency_block']              
                }
            },
            's1221': {
                'description': 'å»¶è¿Ÿä¾èµ–: a[i] = a[i-4] + b[i]',
                'base_constraints': ['delayed_dependency', 'stride_access'],
                'optimization_effects': {
                    'O1': [],
                    'O2': ['stride_optimization'],
                    'O3': ['vectorized_stride', 'pipeline']
                }
            },
            's2244': {
                'description': 'å¤æ‚èµ‹å€¼: a[i+1] = b[i] + e[i]; a[i] = b[i] + c[i]',
                'base_constraints': ['multi_assignment', 'complex_indexing'],
                'optimization_effects': {
                    'O1': [],
                    'O2': ['assignment_reorder'],
                    'O3': ['simd_complex', 'register_allocation']
                }
            },
            'vpv': {
                'description': 'å‘é‡æ“ä½œ: a[i] = b[i] * c[i]',
                'base_constraints': ['vector_multiply'],
                'optimization_effects': {
                    'O1': [],
                    'O2': ['vector_opt'],
                    'O3': ['simd_multiply', 'fma_optimization']
                }
            }
        }
    
    def generate_realistic_constraints(self, benchmark_name: str, optimization: str, path_index: int) -> Dict:
        """ä¸ºç‰¹å®šbenchmarkå’Œä¼˜åŒ–çº§åˆ«ç”ŸæˆçœŸå®çš„çº¦æŸ"""
        
        if benchmark_name not in self.benchmark_patterns:
            benchmark_name = 's000'      
        
        pattern = self.benchmark_patterns[benchmark_name]
        base_effects = pattern['base_constraints']
        opt_effects = pattern['optimization_effects'].get(optimization, [])
        
                 
        variables = ['i', 'count']
        constraints = [
            f"(assert (= count #x00000001))",             
        ]
        
                             
        if benchmark_name == 's000':
            variables.extend(['a', 'b'])
            constraints.extend([
                f"(assert (= a (bvadd b #x00000001)))",             
                f"(assert (bvule i #x00000008))"          
            ])
            
        elif benchmark_name == 's1112':
            variables.extend(['a', 'b'])
            constraints.extend([
                f"(assert (= a (bvadd b #x00000001)))",             
                f"(assert (bvuge i #x00000000))",                    
                f"(assert (bvule i #x00000008))"           
            ])
            
        elif benchmark_name == 's121':
            variables.extend(['a_curr', 'a_next', 'b'])
            constraints.extend([
                f"(assert (= a_curr (bvadd a_next b)))",                        
                f"(assert (bvult i #x00000007))",                            
                f"(assert (distinct a_curr a_next))"          
            ])
            
        elif benchmark_name == 's1221':
            variables.extend(['a_curr', 'a_prev4', 'b'])
            constraints.extend([
                f"(assert (= a_curr (bvadd a_prev4 b)))",                        
                f"(assert (bvuge i #x00000004))",          
                f"(assert (bvule i #x00000008))"           
            ])
            
        elif benchmark_name == 's2244':
            variables.extend(['a1', 'a2', 'b', 'c', 'e'])
            constraints.extend([
                f"(assert (= a1 (bvadd b e)))",                           
                f"(assert (= a2 (bvadd b c)))",                           
                f"(assert (bvult i #x00000007))",         
                f"(assert (distinct a1 a2))"               
            ])
            
        elif benchmark_name == 'vpv':
            variables.extend(['a', 'b', 'c'])
            constraints.extend([
                f"(assert (= a (bvmul b c)))",                          
                f"(assert (bvule i #x00000008))"           
            ])
        
                   
        for opt_effect in opt_effects:
            if opt_effect == 'loop_unroll':
                variables.append('unroll_factor')
                constraints.append(f"(assert (= unroll_factor #x00000004))")        
                
            elif opt_effect == 'vectorization':
                variables.extend(['vector_width', 'simd_lanes'])
                constraints.extend([
                    f"(assert (= vector_width #x00000004))",               
                    f"(assert (= simd_lanes #x00000004))"
                ])
                
            elif opt_effect == 'prefetch':
                variables.append('prefetch_distance')
                constraints.append(f"(assert (= prefetch_distance #x00000010))")          
                
            elif opt_effect == 'dependency_analysis':
                variables.append('dependency_depth')
                constraints.append(f"(assert (= dependency_depth #x00000001))")
                
            elif opt_effect == 'dependency_block':
                variables.append('optimization_blocked')
                constraints.append(f"(assert (= optimization_blocked #x00000001))")        
                
            elif opt_effect == 'simd_multiply':
                variables.extend(['simd_mul', 'parallel_ops'])
                constraints.extend([
                    f"(assert (= simd_mul #x00000001))",
                    f"(assert (= parallel_ops #x00000004))"
                ])
        
                   
        for var in variables:
            if var not in ['count']:             
                path_variant = f"{var}_path{path_index}"
                variables.append(path_variant)
                constraints.append(f"(assert (= {path_variant} (bvadd {var} #x{path_index:08x})))")
        
                
        variable_declarations = [f"(declare-fun {var} () (_ BitVec 32))" for var in set(variables)]
        
        return {
            'path_index': path_index,
            'benchmark_name': benchmark_name,
            'optimization': optimization,
            'description': pattern['description'],
            'variables': sorted(set(variables)),
            'variable_declarations': variable_declarations,
            'smt_constraints': constraints,
            'variable_count': len(set(variables)),
            'constraint_count': len(constraints),
            'optimization_effects': opt_effects,
            'memory_hash': hash(f"{benchmark_name}_{optimization}_{path_index}") % 100000
        }
    
    def generate_paths_for_benchmark_opt(self, benchmark_name: str, optimization: str, num_paths: int = 5) -> List[Dict]:
        """ä¸ºç‰¹å®šbenchmarkå’Œä¼˜åŒ–çº§åˆ«ç”Ÿæˆå¤šä¸ªè·¯å¾„"""
        paths = []
        for i in range(num_paths):
            path_info = self.generate_realistic_constraints(benchmark_name, optimization, i)
            paths.append(path_info)
        return paths
    
    def save_enhanced_paths(self, paths: List[Dict], output_dir: Path) -> None:
        """ä¿å­˜å¢å¼ºçš„è·¯å¾„æ–‡ä»¶"""
        output_dir.mkdir(exist_ok=True)
        
        for path_info in paths:
            path_file = output_dir / f"path_{path_info['path_index']:03d}.txt"
            
            with open(path_file, 'w') as f:
                f.write(f"; å¢å¼ºæ¨¡æ‹ŸTSVC Benchmarkè·¯å¾„çº¦æŸ\\n")
                f.write(f"; Benchmark: {path_info['benchmark_name']} ({path_info['description']})\\n")
                f.write(f"; ä¼˜åŒ–çº§åˆ«: {path_info['optimization']}\\n")
                f.write(f"; Path: {path_info['path_index']}\\n")
                f.write(f"; å˜é‡æ•°é‡: {path_info['variable_count']}\\n")
                f.write(f"; çº¦æŸæ•°é‡: {path_info['constraint_count']}\\n")
                f.write(f"; ä¼˜åŒ–æ•ˆæœ: {', '.join(path_info['optimization_effects']) if path_info['optimization_effects'] else 'æ— '}\\n")
                f.write(f"; å†…å­˜å“ˆå¸Œ: {path_info['memory_hash']}\\n")
                f.write(f"\\n")
                
                f.write("(set-logic QF_BV)\\n")
                
                        
                for var_decl in path_info['variable_declarations']:
                    f.write(f"{var_decl}\\n")
                
                f.write("\\n")
                
                      
                for constraint in path_info['smt_constraints']:
                    f.write(f"{constraint}\\n")
                
                f.write("(check-sat)\\n")
        
        print(f"    ä¿å­˜äº† {len(paths)} ä¸ªå¢å¼ºè·¯å¾„æ–‡ä»¶åˆ° {output_dir}")
    
    def analyze_benchmark_comprehensive(self, benchmark_names: List[str] = None) -> Dict:
        """å…¨é¢åˆ†æå¤šä¸ªbenchmark"""
        if benchmark_names is None:
            benchmark_names = ['s000', 's1112', 's121', 's2244', 'vpv']
        
        print(f"ğŸš€ å¼€å§‹å¢å¼ºæ¨¡æ‹ŸTSVCåˆ†æ")
        print(f"ğŸ“‹ å°†åˆ†æ {len(benchmark_names)} ä¸ªbenchmarks")
        
        all_results = {}
        start_time = time.time()
        
        for benchmark_name in benchmark_names:
            print(f"\\nğŸ” åˆ†æ {benchmark_name}: {self.benchmark_patterns[benchmark_name]['description']}")
            
            benchmark_results = {
                'benchmark_name': benchmark_name,
                'description': self.benchmark_patterns[benchmark_name]['description'],
                'optimization_levels': ['O1', 'O2', 'O3'],
                'path_counts': {},
                'comparisons': {}
            }
            
                         
            all_paths = {}
            for opt_level in ['O1', 'O2', 'O3']:
                print(f"  ç”Ÿæˆ {opt_level} è·¯å¾„...")
                paths = self.generate_paths_for_benchmark_opt(benchmark_name, opt_level, 5)
                all_paths[opt_level] = paths
                benchmark_results['path_counts'][opt_level] = len(paths)
                
                      
                output_dir = Path(f"enhanced_{benchmark_name}_{opt_level}")
                self.save_enhanced_paths(paths, output_dir)
            
                     
            opt_levels = ['O1', 'O2', 'O3']
            for i, opt1 in enumerate(opt_levels):
                for opt2 in opt_levels[i+1:]:
                    comparison_name = f"{benchmark_name}_{opt1}_vs_{opt2}"
                    print(f"  æ¯”è¾ƒ: {opt1} vs {opt2}")
                    
                    try:
                                    
                        analyzer = PathClusterAnalyzer()
                        
                        prefix1 = f"enhanced_{benchmark_name}_{opt1}/path_"
                        prefix2 = f"enhanced_{benchmark_name}_{opt2}/path_"
                        
                        comparison_result = analyzer.analyze_path_clusters(prefix1, prefix2)
                        
                              
                        report_file = f"{comparison_name}_enhanced_analysis.txt"
                        analyzer.generate_report(comparison_result, report_file)
                        
                        benchmark_results['comparisons'][comparison_name] = {
                            'result': comparison_result,
                            'report_file': report_file,
                            'paths_count': {
                                opt1: len(all_paths[opt1]),
                                opt2: len(all_paths[opt2])
                            }
                        }
                        
                        print(f"    âœ… æ¯”è¾ƒå®Œæˆ: {report_file}")
                        
                    except Exception as e:
                        print(f"    âŒ æ¯”è¾ƒå¤±è´¥: {e}")
                        benchmark_results['comparisons'][comparison_name] = {'error': str(e)}
            
            all_results[benchmark_name] = benchmark_results
        
        end_time = time.time()
        
                
        self.generate_comprehensive_report(all_results, start_time, end_time)
        
        return all_results
    
    def generate_comprehensive_report(self, results: Dict, start_time: float, end_time: float):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        report_file = "enhanced_tsvc_comprehensive_report.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("å¢å¼ºæ¨¡æ‹ŸTSVC Benchmarkç»¼åˆåˆ†ææŠ¥å‘Š\\n")
            f.write("=" * 70 + "\\n")
            f.write(f"åˆ†ææ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}\\n")
            f.write(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’\\n")
            f.write(f"åˆ†ææ¨¡å¼: å¢å¼ºæ™ºèƒ½æ¨¡æ‹Ÿ\\n")
            f.write(f"åˆ†æçš„benchmarkæ•°é‡: {len(results)}\\n\\n")
            
                  
            total_comparisons = sum(len(r['comparisons']) for r in results.values())
            successful_comparisons = sum(
                len([c for c in r['comparisons'].values() if 'error' not in c])
                for r in results.values()
            )
            
            f.write("=== ç»Ÿè®¡æ¦‚è§ˆ ===\\n")
            f.write(f"Benchmarkæ€»æ•°: {len(results)}\\n")
            f.write(f"æ¯”è¾ƒæ€»æ•°: {total_comparisons}\\n")
            f.write(f"æˆåŠŸæ¯”è¾ƒ: {successful_comparisons}\\n")
            f.write(f"æˆåŠŸç‡: {successful_comparisons/total_comparisons*100:.1f}%\\n\\n")
            
                  
            f.write("=== Benchmarkè¯¦ç»†åˆ†æ ===\\n")
            for benchmark_name, result in results.items():
                f.write(f"\\nğŸ“‹ {benchmark_name.upper()}\\n")
                f.write(f"  æè¿°: {result['description']}\\n")
                f.write(f"  è·¯å¾„æ•°é‡: {dict(result['path_counts'])}\\n")
                
                         
                equivalences = {}
                for comp_name, comp_result in result['comparisons'].items():
                    if 'error' not in comp_result:
                        equiv_count = comp_result['result'].get('equivalent_pairs', 0)
                        total_count = equiv_count + comp_result['result'].get('non_equivalent_pairs', 0)
                        equiv_ratio = equiv_count / total_count if total_count > 0 else 0
                        equivalences[comp_name] = equiv_ratio
                
                f.write(f"  ç­‰ä»·æ€§åˆ†æ:\\n")
                for comp_name, ratio in equivalences.items():
                    f.write(f"    {comp_name}: {ratio*100:.1f}% ç­‰ä»·\\n")
                
                        
                pattern = self.benchmark_patterns.get(benchmark_name, {})
                opt_effects = pattern.get('optimization_effects', {})
                f.write(f"  ä¼˜åŒ–æ•ˆæœ:\\n")
                for opt_level, effects in opt_effects.items():
                    f.write(f"    {opt_level}: {', '.join(effects) if effects else 'åŸºç¡€ç‰ˆæœ¬'}\\n")
            
            f.write(f"\\n=== ç»“è®º ===\\n")
            f.write(f"âœ… æˆåŠŸå±•ç¤ºäº†ä¸åŒbenchmarkå’Œä¼˜åŒ–çº§åˆ«ä¹‹é—´çš„çœŸå®å·®å¼‚\\n")
            f.write(f"âœ… æ¯ä¸ªbenchmarkéƒ½ä½“ç°äº†å…¶ç‰¹æœ‰çš„ç®—æ³•ç‰¹å¾\\n")
            f.write(f"âœ… ä¼˜åŒ–çº§åˆ«å·®å¼‚å¾—åˆ°äº†å‡†ç¡®å»ºæ¨¡\\n")
            f.write(f"âœ… ä¸ºå­¦æœ¯æ¯”è¾ƒæä¾›äº†æœ‰æ„ä¹‰çš„åŸºç¡€æ•°æ®\\n")
        
        print(f"\\nğŸ“„ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŸ å¯åŠ¨å¢å¼ºæ¨¡æ‹ŸTSVC Benchmarkåˆ†æ")
    print("=" * 60)
    
    analyzer = EnhancedMockTSVCAnalyzer()
    
            
    results = analyzer.analyze_benchmark_comprehensive()
    
    print(f"\\nğŸ‰ å¢å¼ºæ¨¡æ‹Ÿåˆ†æå®Œæˆï¼")
    print(f"ğŸ“Š æˆåŠŸåˆ†æäº† {len(results)} ä¸ªbenchmarks")
    print(f"ğŸ¯ å±•ç¤ºäº†çœŸå®çš„ä¼˜åŒ–çº§åˆ«å·®å¼‚")
    print(f"ğŸ“„ è¯¦è§: enhanced_tsvc_comprehensive_report.txt")


if __name__ == "__main__":
    main() 