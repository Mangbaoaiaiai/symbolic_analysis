                      
"""
Benchmarkåˆ†æè‡ªåŠ¨åŒ–è„šæœ¬

è‡ªåŠ¨è¿è¡Œç¬¦å·æ‰§è¡Œå’Œè¯­ä¹‰ç­‰ä»·æ€§åˆ†æçš„å®Œæ•´æµç¨‹
ä½¿ç”¨æ”¹è¿›çš„ç¬¦å·æ‰§è¡Œè„šæœ¬å¤„ç†æ²¡æœ‰è¾“å…¥çš„benchmarkç¨‹åº
"""

import os
import sys
import subprocess
import time
import argparse

def run_command(cmd, description):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºè¿›åº¦"""
    print(f"\n{'='*60}")
    print(f"æ­£åœ¨æ‰§è¡Œ: {description}")
    print(f"å‘½ä»¤: {cmd}")
    print(f"{'='*60}")
    
    start_time = time.time()
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    end_time = time.time()
    
    print(f"æ‰§è¡Œæ—¶é—´: {end_time - start_time:.2f} ç§’")
    
    if result.returncode == 0:
        print("âœ“ æ‰§è¡ŒæˆåŠŸ")
        if result.stdout:
            print("è¾“å‡º:")
            print(result.stdout)
    else:
        print("âŒ æ‰§è¡Œå¤±è´¥")
        print("é”™è¯¯è¾“å‡º:")
        print(result.stderr)
        return False
    
    return True

def analyze_benchmark(benchmark_dir, timeout=120, use_improved=True):
    """åˆ†ææ•´ä¸ªbenchmark"""
    print(f"å¼€å§‹åˆ†æbenchmark: {benchmark_dir}")
    
    if not os.path.exists(benchmark_dir):
        print(f"é”™è¯¯: benchmarkç›®å½• '{benchmark_dir}' ä¸å­˜åœ¨")
        return False
    
              
    se_script = "se_script_improved.py" if use_improved else "se_script.py"
    
                 
    print(f"\nç¬¬ä¸€æ­¥: ä½¿ç”¨{se_script}å¯¹æ‰€æœ‰ä¼˜åŒ–ç­‰çº§è¿è¡Œç¬¦å·æ‰§è¡Œ")
    se_cmd = f"python {se_script} --benchmark {benchmark_dir} --timeout {timeout}"
    if not run_command(se_cmd, "æ”¹è¿›çš„ç¬¦å·æ‰§è¡Œåˆ†æ"):
        return False
    
                    
    print("\nç¬¬äºŒæ­¥: è¿›è¡Œè¯­ä¹‰ç­‰ä»·æ€§åˆ†æ")
    equiv_cmd = f"python semantic_equivalence_analyzer.py --benchmark {benchmark_dir}"
    if not run_command(equiv_cmd, "è¯­ä¹‰ç­‰ä»·æ€§åˆ†æ"):
        return False
    
                 
    print("\nç¬¬ä¸‰æ­¥: æ˜¾ç¤ºåˆ†æç»“æœ")
    summary_file = os.path.join(benchmark_dir, "optimization_equivalence_summary.txt")
    if os.path.exists(summary_file):
        print(f"åˆ†æå®Œæˆï¼ç»“æœæ‘˜è¦:")
        with open(summary_file, 'r', encoding='utf-8') as f:
            content = f.read()
            print(content)
            
                   
        if "âœ“ æ‰€æœ‰ä¼˜åŒ–ç­‰çº§åœ¨è¯­ä¹‰ä¸Šå®Œå…¨ç­‰ä»·" in content:
            print("\nğŸ‰ ç»“è®º: æ‰€æœ‰ä¼˜åŒ–ç­‰çº§ä¿æŒè¯­ä¹‰ç­‰ä»·ï¼Œç¼–è¯‘å™¨ä¼˜åŒ–æ˜¯å®‰å…¨çš„ï¼")
        elif "âš  å¤§éƒ¨åˆ†ä¼˜åŒ–ç­‰çº§åœ¨è¯­ä¹‰ä¸Šç­‰ä»·" in content:
            print("\nâš ï¸  ç»“è®º: å¤§éƒ¨åˆ†ä¼˜åŒ–ç­‰ä»·ï¼Œä½†éœ€è¦æ£€æŸ¥å·®å¼‚éƒ¨åˆ†")
        else:
            print("\nğŸ” ç»“è®º: å‘ç°ä¼˜åŒ–å·®å¼‚ï¼Œè¿™å¯èƒ½è¡¨æ˜ï¼š")
            print("   1. ç¼–è¯‘å™¨ä¼˜åŒ–æ”¹å˜äº†ç¨‹åºè¡Œä¸º")
            print("   2. éœ€è¦è¿›ä¸€æ­¥åˆ†æå…·ä½“å·®å¼‚")
            print("   3. å¯¹äºbenchmarkç¨‹åºï¼Œè¿™ç§å·®å¼‚å¯èƒ½æ˜¯æ­£å¸¸çš„")
            
    else:
        print("æœªæ‰¾åˆ°åˆ†ææ‘˜è¦æ–‡ä»¶")
    
    return True

def main():
    parser = argparse.ArgumentParser(description='Benchmarkåˆ†æè‡ªåŠ¨åŒ–å·¥å…·')
    parser.add_argument('benchmark_dir', help='benchmarkç›®å½•è·¯å¾„')
    parser.add_argument('--timeout', type=int, default=120, help='ç¬¦å·æ‰§è¡Œè¶…æ—¶æ—¶é—´(ç§’)')
    parser.add_argument('--step', choices=['se', 'equiv', 'all'], default='all', 
                       help='æ‰§è¡Œçš„æ­¥éª¤: se(ä»…ç¬¦å·æ‰§è¡Œ), equiv(ä»…ç­‰ä»·æ€§åˆ†æ), all(å…¨éƒ¨)')
    parser.add_argument('--use-original', action='store_true', 
                       help='ä½¿ç”¨åŸå§‹ç¬¦å·æ‰§è¡Œè„šæœ¬(é€‚åˆæœ‰è¾“å…¥çš„ç¨‹åº)')
    
    args = parser.parse_args()
    
    use_improved = not args.use_original
    se_script = "se_script.py" if args.use_original else "se_script_improved.py"
    
    if args.step in ['se', 'all']:
                
        print(f"ä½¿ç”¨è„šæœ¬: {se_script}")
        se_cmd = f"python {se_script} --benchmark {args.benchmark_dir} --timeout {args.timeout}"
        if not run_command(se_cmd, "ç¬¦å·æ‰§è¡Œåˆ†æ"):
            return
    
    if args.step in ['equiv', 'all']:
                 
        equiv_cmd = f"python semantic_equivalence_analyzer.py --benchmark {args.benchmark_dir}"
        if not run_command(equiv_cmd, "è¯­ä¹‰ç­‰ä»·æ€§åˆ†æ"):
            return
    
          
    summary_file = os.path.join(args.benchmark_dir, "optimization_equivalence_summary.txt")
    if os.path.exists(summary_file):
        print(f"\nåˆ†æå®Œæˆï¼ç»“æœæ‘˜è¦åœ¨: {summary_file}")
        
                 
        print(f"\nç”Ÿæˆçš„å…³é”®æ–‡ä»¶:")
        
                
        path_files = subprocess.run(f"find {args.benchmark_dir} -name '*_path_*.txt' -type f 2>/dev/null", 
                                   shell=True, capture_output=True, text=True)
        if path_files.stdout:
            print("è·¯å¾„çº¦æŸæ–‡ä»¶:")
            for file in sorted(path_files.stdout.strip().split('\n')):
                if file:
                    print(f"  {file}")
        
                 
        report_files = subprocess.run(f"find {args.benchmark_dir} -name 'equivalence_report_*.txt' -type f 2>/dev/null", 
                                     shell=True, capture_output=True, text=True)
        if report_files.stdout:
            print("ç­‰ä»·æ€§åˆ†ææŠ¥å‘Š:")
            for file in sorted(report_files.stdout.strip().split('\n')):
                if file:
                    print(f"  {file}")
                    
              
        summary_files = subprocess.run(f"find {args.benchmark_dir} -name '*summary*.txt' -type f 2>/dev/null", 
                                      shell=True, capture_output=True, text=True)
        if summary_files.stdout:
            print("æ‘˜è¦æŠ¥å‘Š:")
            for file in sorted(summary_files.stdout.strip().split('\n')):
                if file:
                    print(f"  {file}")

if __name__ == "__main__":
    main() 