#!/usr/bin/env python3
"""
é›†æˆçš„ç¬¦å·æ‰§è¡Œå’Œç­‰ä»·æ€§åˆ†æå·¥å…·

ç»“åˆç¬¦å·æ‰§è¡Œå’Œè¯­ä¹‰ç­‰ä»·æ€§åˆ†æï¼Œæä¾›å®Œæ•´çš„åˆ†ææµç¨‹å’Œè¯¦ç»†æ—¶é—´ç»Ÿè®¡
"""

import os
import time
import datetime
import argparse
import glob
from collections import defaultdict
import re # Added for ProgramSpecificEquivalenceAnalyzer

# å¯¼å…¥ç°æœ‰æ¨¡å—
from se_script import BenchmarkAnalyzer, ImprovedPathAnalyzer
from semantic_equivalence_analyzer import BenchmarkEquivalenceAnalyzer, PathClusterAnalyzer

class ProgramSpecificEquivalenceAnalyzer:
    """é’ˆå¯¹ç‰¹å®šç¨‹åºçš„ç­‰ä»·æ€§åˆ†æå™¨"""
    
    def __init__(self, benchmark_dir, program_name):
        self.benchmark_dir = benchmark_dir
        self.program_name = program_name
        self.analyzer = PathClusterAnalyzer()
        
    def find_program_optimization_levels(self):
        """æŸ¥æ‰¾æŒ‡å®šç¨‹åºçš„æ‰€æœ‰ä¼˜åŒ–ç­‰çº§è·¯å¾„æ–‡ä»¶"""
        optimization_levels = {}
        
        # åªæŸ¥æ‰¾æŒ‡å®šç¨‹åºçš„è·¯å¾„æ–‡ä»¶
        pattern = os.path.join(self.benchmark_dir, f"{self.program_name}_O*_path_*.txt")
        path_files = glob.glob(pattern)
        
        print(f"æŸ¥æ‰¾æ¨¡å¼: {pattern}")
        print(f"æ‰¾åˆ°è·¯å¾„æ–‡ä»¶: {len(path_files)} ä¸ª")
        
        for file_path in path_files:
            basename = os.path.basename(file_path)
            # æå–ä¼˜åŒ–ç­‰çº§å‰ç¼€ (ä¾‹å¦‚: s121_O1_path_1.txt -> s121_O1)
            match = re.match(rf'{self.program_name}_(O[0-3])_path_\d+\.txt', basename)
            if match:
                opt_level = match.group(1)
                opt_prefix = f"{self.program_name}_{opt_level}"
                if opt_prefix not in optimization_levels:
                    optimization_levels[opt_prefix] = []
                optimization_levels[opt_prefix].append(file_path)
        
        return optimization_levels
    
    def compare_program_optimization_pairs(self):
        """æ¯”è¾ƒæŒ‡å®šç¨‹åºçš„æ‰€æœ‰ä¼˜åŒ–ç­‰çº§å¯¹"""
        
        optimization_levels = self.find_program_optimization_levels()
        
        if len(optimization_levels) < 2:
            print(f"ç¨‹åº {self.program_name} çš„ä¼˜åŒ–ç­‰çº§æ•°é‡ä¸è¶³ä»¥è¿›è¡Œæ¯”è¾ƒ (æ‰¾åˆ° {len(optimization_levels)} ä¸ª)")
            print("å‘ç°çš„ä¼˜åŒ–ç­‰çº§:", list(optimization_levels.keys()))
            return None
        
        print(f"ç¨‹åº {self.program_name} å‘ç° {len(optimization_levels)} ä¸ªä¼˜åŒ–ç­‰çº§:")
        for opt_level in sorted(optimization_levels.keys()):
            print(f"  {opt_level}: {len(optimization_levels[opt_level])} ä¸ªè·¯å¾„æ–‡ä»¶")
        
        # ç”Ÿæˆæ‰€æœ‰ä¼˜åŒ–ç­‰çº§å¯¹
        opt_levels = sorted(optimization_levels.keys())
        comparison_results = {}
        
        for i, opt1 in enumerate(opt_levels):
            for j, opt2 in enumerate(opt_levels):
                if i >= j:  # é¿å…é‡å¤æ¯”è¾ƒå’Œè‡ªæ¯”è¾ƒ
                    continue
                
                print(f"\n{'='*60}")
                print(f"æ¯”è¾ƒ {opt1} vs {opt2}")
                print(f"{'='*60}")
                
                # ä½¿ç”¨è·¯å¾„å‰ç¼€è€Œä¸æ˜¯å®Œæ•´ç›®å½•
                prefix1 = os.path.join(self.benchmark_dir, f"{opt1}_path_")
                prefix2 = os.path.join(self.benchmark_dir, f"{opt2}_path_")
                
                try:
                    results = self.analyzer.analyze_path_clusters(prefix1, prefix2)
                    comparison_results[(opt1, opt2)] = results
                    
                    # ç”Ÿæˆå•ç‹¬çš„æŠ¥å‘Š
                    report_file = os.path.join(
                        self.benchmark_dir, 
                        f"equivalence_report_{opt1}_vs_{opt2}.txt"
                    )
                    self.analyzer.generate_report(results, report_file)
                    print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
                    
                except Exception as e:
                    print(f"æ¯”è¾ƒ {opt1} vs {opt2} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    comparison_results[(opt1, opt2)] = {
                        'error': str(e),
                        'status': 'failed'
                    }
        
        # ç”Ÿæˆé’ˆå¯¹ç¨‹åºçš„æ‘˜è¦æŠ¥å‘Š
        self.generate_program_summary_report(comparison_results)
        
        return comparison_results
    
    def generate_program_summary_report(self, comparison_results):
        """ç”Ÿæˆé’ˆå¯¹ç‰¹å®šç¨‹åºçš„æ‘˜è¦æŠ¥å‘Š"""
        summary_file = os.path.join(self.benchmark_dir, f"{self.program_name}_equivalence_summary.txt")
        
        with open(summary_file, "w", encoding='utf-8') as f:
            f.write(f"ç¨‹åº {self.program_name} ä¼˜åŒ–ç­‰çº§ç­‰ä»·æ€§åˆ†ææ‘˜è¦\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"åˆ†æç›®å½•: {self.benchmark_dir}\n")
            f.write(f"ç›®æ ‡ç¨‹åº: {self.program_name}\n")
            f.write(f"æ¯”è¾ƒçš„ä¼˜åŒ–ç­‰çº§å¯¹æ•°: {len(comparison_results)}\n\n")
            
            for (opt1, opt2), results in comparison_results.items():
                f.write(f"æ¯”è¾ƒ {opt1} vs {opt2}:\n")
                if 'error' in results:
                    f.write(f"  çŠ¶æ€: å¤±è´¥\n")
                    f.write(f"  é”™è¯¯: {results['error']}\n")
                else:
                    f.write(f"  çŠ¶æ€: æˆåŠŸ\n")
                    if 'equivalent_pairs' in results:
                        f.write(f"  ç­‰ä»·è·¯å¾„å¯¹: {len(results['equivalent_pairs'])}\n")
                    if 'non_equivalent_pairs' in results:
                        f.write(f"  éç­‰ä»·è·¯å¾„å¯¹: {len(results['non_equivalent_pairs'])}\n")
                f.write("\n")
        
        print(f"ç¨‹åºç‰¹å®šæ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_file}")

class IntegratedAnalysisFramework:
    """é›†æˆåˆ†ææ¡†æ¶"""
    
    def __init__(self, benchmark_dir, timeout=120, force_rerun=False, target_program=None):
        self.benchmark_dir = benchmark_dir
        self.timeout = timeout
        self.force_rerun = force_rerun
        self.target_program = target_program  # æ·»åŠ ç›®æ ‡ç¨‹åºå‚æ•°
        self.timing_data = {
            'total_start_time': None,
            'total_end_time': None,
            'symbolic_execution': {},
            'equivalence_analysis': {},
            'phase_times': {}
        }
        
    def run_complete_analysis(self, binary_patterns=None):
        """è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹"""
        print("=" * 80)
        print("å¼€å§‹é›†æˆåˆ†æï¼šç¬¦å·æ‰§è¡Œ + ç­‰ä»·æ€§åˆ†æ")
        if self.target_program:
            print(f"ç›®æ ‡ç¨‹åº: {self.target_program}")
        print("=" * 80)
        
        self.timing_data['total_start_time'] = time.time()
        
        # é˜¶æ®µ1ï¼šç¬¦å·æ‰§è¡Œ
        print(f"\nğŸ“Š é˜¶æ®µ1ï¼šç¬¦å·æ‰§è¡Œ")
        print("-" * 50)
        se_start_time = time.time()
        
        if binary_patterns:
            # åˆ†ææŒ‡å®šçš„äºŒè¿›åˆ¶æ–‡ä»¶
            se_results = self.run_targeted_symbolic_execution(binary_patterns)
        else:
            # æ‰¹é‡åˆ†æ
            se_results = self.run_batch_symbolic_execution()
        
        se_end_time = time.time()
        se_duration = se_end_time - se_start_time
        
        self.timing_data['phase_times']['symbolic_execution'] = se_duration
        self.timing_data['symbolic_execution'] = se_results
        
        print(f"\nâœ… ç¬¦å·æ‰§è¡Œé˜¶æ®µå®Œæˆï¼Œè€—æ—¶: {se_duration:.3f} ç§’")
        
        # é˜¶æ®µ2ï¼šç­‰ä»·æ€§åˆ†æ
        print(f"\nğŸ” é˜¶æ®µ2ï¼šç­‰ä»·æ€§åˆ†æ")
        print("-" * 50)
        eq_start_time = time.time()
        
        eq_results = self.run_equivalence_analysis()
        
        eq_end_time = time.time()
        eq_duration = eq_end_time - eq_start_time
        
        self.timing_data['phase_times']['equivalence_analysis'] = eq_duration
        self.timing_data['equivalence_analysis'] = eq_results
        
        print(f"\nâœ… ç­‰ä»·æ€§åˆ†æé˜¶æ®µå®Œæˆï¼Œè€—æ—¶: {eq_duration:.3f} ç§’")
        
        # å®Œæˆåˆ†æ
        self.timing_data['total_end_time'] = time.time()
        total_duration = self.timing_data['total_end_time'] - self.timing_data['total_start_time']
        self.timing_data['phase_times']['total'] = total_duration
        
        print(f"\nğŸ‰ å®Œæ•´åˆ†ææµç¨‹å®Œæˆï¼Œæ€»è€—æ—¶: {total_duration:.3f} ç§’")
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_comprehensive_report()
        
        return {
            'symbolic_execution_results': se_results,
            'equivalence_analysis_results': eq_results,
            'timing_data': self.timing_data
        }
    
    def run_batch_symbolic_execution(self):
        """æ‰¹é‡è¿è¡Œç¬¦å·æ‰§è¡Œ"""
        print("æ­£åœ¨æŸ¥æ‰¾äºŒè¿›åˆ¶æ–‡ä»¶...")
        
        analyzer = BenchmarkAnalyzer(self.benchmark_dir, self.timeout)
        results = analyzer.analyze_all_binaries()
        
        # æ”¶é›†è¯¦ç»†çš„æ—¶é—´ä¿¡æ¯
        se_timing = {}
        for binary_name, paths in results.items():
            # è¿™é‡Œåº”è¯¥ä»analyzerä¸­è·å–æ—¶é—´ä¿¡æ¯ï¼Œä½†ç°æœ‰ä»£ç æ²¡æœ‰è¿”å›
            # æˆ‘ä»¬å¯ä»¥ä¼°ç®—æ¯ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶çš„åˆ†ææ—¶é—´
            se_timing[binary_name] = {
                'path_count': len(paths),
                'estimated_time': len(paths) * 0.1  # ä¼°ç®—æ—¶é—´
            }
        
        analyzer.generate_summary_report()
        
        return {
            'results': results,
            'timing': se_timing,
            'summary_file': os.path.join(self.benchmark_dir, "symbolic_execution_summary.txt")
        }
    
    def run_targeted_symbolic_execution(self, binary_patterns):
        """è¿è¡Œé’ˆå¯¹ç‰¹å®šäºŒè¿›åˆ¶æ–‡ä»¶çš„ç¬¦å·æ‰§è¡Œ"""
        results = {}
        se_timing = {}
        
        for pattern in binary_patterns:
            # æŸ¥æ‰¾äºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆä¸æ˜¯è·¯å¾„æ–‡ä»¶ï¼‰
            all_files = glob.glob(os.path.join(self.benchmark_dir, pattern))
            binary_files = [f for f in all_files if not f.endswith('.txt') and not f.endswith('.c')]
            
            for binary_path in binary_files:
                print(f"æ£€æŸ¥äºŒè¿›åˆ¶æ–‡ä»¶: {binary_path}")
                
                basename = os.path.basename(binary_path)
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è·¯å¾„æ–‡ä»¶ï¼ˆåœ¨å½“å‰ç›®å½•å’Œbenchmarkç›®å½•ä¸­æŸ¥æ‰¾ï¼‰
                current_dir_paths = glob.glob(f"{basename}_path_*.txt")
                benchmark_dir_paths = glob.glob(os.path.join(self.benchmark_dir, f"{basename}_path_*.txt"))
                
                existing_paths = current_dir_paths + benchmark_dir_paths
                
                if existing_paths and not self.force_rerun:
                    print(f"å‘ç°å·²å­˜åœ¨çš„è·¯å¾„æ–‡ä»¶: {len(existing_paths)} ä¸ª")
                    print("è·³è¿‡ç¬¦å·æ‰§è¡Œï¼Œä½¿ç”¨ç°æœ‰è·¯å¾„æ–‡ä»¶")
                    
                    # æ¨¡æ‹Ÿç»“æœç»“æ„
                    mock_paths = []
                    for i, path_file in enumerate(existing_paths):
                        mock_paths.append({
                            'index': i + 1,
                            'signature': {'output': f'è·¯å¾„{i+1}'},
                            'smt_constraints': f'æ¥è‡ªæ–‡ä»¶: {path_file}',
                            'state': None
                        })
                    
                    results[basename] = mock_paths
                    se_timing[basename] = {
                        'path_count': len(existing_paths),
                        'actual_time': 0.0,  # æ²¡æœ‰å®é™…æ‰§è¡Œæ—¶é—´
                        'skipped': True
                    }
                    
                    print(f"ä½¿ç”¨ç°æœ‰è·¯å¾„: {basename} - {len(existing_paths)} æ¡è·¯å¾„")
                else:
                    if existing_paths and self.force_rerun:
                        print(f"å¼ºåˆ¶é‡æ–°æ‰§è¡Œæ¨¡å¼: åˆ é™¤ {len(existing_paths)} ä¸ªç°æœ‰è·¯å¾„æ–‡ä»¶")
                        for path_file in existing_paths:
                            try:
                                os.remove(path_file)
                                print(f"  åˆ é™¤: {path_file}")
                            except OSError as e:
                                print(f"  åˆ é™¤å¤±è´¥ {path_file}: {e}")
                        print("ç°æœ‰è·¯å¾„æ–‡ä»¶å·²åˆ é™¤")
                    
                    print(f"åˆ†æäºŒè¿›åˆ¶æ–‡ä»¶: {binary_path}")
                    
                    binary_start = time.time()
                    
                    analyzer = ImprovedPathAnalyzer(binary_path, basename, self.timeout)
                    paths = analyzer.run_symbolic_execution()
                    
                    binary_end = time.time()
                    binary_duration = binary_end - binary_start
                    
                    results[basename] = paths
                    se_timing[basename] = {
                        'path_count': len(paths),
                        'actual_time': binary_duration,
                        'skipped': False
                    }
                    
                    print(f"å®Œæˆ {basename}: {len(paths)} æ¡è·¯å¾„ï¼Œè€—æ—¶ {binary_duration:.3f} ç§’")
        
        return {
            'results': results,
            'timing': se_timing
        }
    
    def run_equivalence_analysis(self):
        """è¿è¡Œç­‰ä»·æ€§åˆ†æ"""
        print("å¼€å§‹ç­‰ä»·æ€§åˆ†æ...")
        
        # æ£€æŸ¥è·¯å¾„æ–‡ä»¶çš„å®é™…ä½ç½®
        current_dir_files = glob.glob("*_path_*.txt")
        benchmark_dir_files = glob.glob(os.path.join(self.benchmark_dir, "*_path_*.txt"))
        
        # å†³å®šåœ¨å“ªä¸ªç›®å½•è¿è¡Œç­‰ä»·æ€§åˆ†æ
        if current_dir_files and not benchmark_dir_files:
            print(f"åœ¨å½“å‰ç›®å½•æ‰¾åˆ°è·¯å¾„æ–‡ä»¶ï¼Œåˆ‡æ¢åˆ†æç›®å½•åˆ°å½“å‰ç›®å½•")
            analysis_dir = "."
        else:
            analysis_dir = self.benchmark_dir
        
        # å¦‚æœæŒ‡å®šäº†ç›®æ ‡ç¨‹åºï¼Œä½¿ç”¨ç¨‹åºç‰¹å®šçš„åˆ†æå™¨
        if self.target_program:
            print(f"ä½¿ç”¨ç¨‹åºç‰¹å®šåˆ†æå™¨åˆ†æ: {self.target_program}")
            program_analyzer = ProgramSpecificEquivalenceAnalyzer(analysis_dir, self.target_program)
            comparison_results = program_analyzer.compare_program_optimization_pairs()
        else:
            print("ä½¿ç”¨é€šç”¨åˆ†æå™¨åˆ†ææ‰€æœ‰ç¨‹åº")
            benchmark_analyzer = BenchmarkEquivalenceAnalyzer(analysis_dir)
            comparison_results = benchmark_analyzer.compare_all_optimization_pairs()
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å¾—æ¯”è¾ƒç»“æœ
        if comparison_results is None:
            error_msg = f"æœªæ‰¾åˆ°è¶³å¤Ÿçš„ä¼˜åŒ–ç­‰çº§æˆ–è·¯å¾„æ–‡ä»¶"
            if self.target_program:
                error_msg += f" (ç›®æ ‡ç¨‹åº: {self.target_program})"
            print(f"âŒ ç­‰ä»·æ€§åˆ†æå¤±è´¥ï¼š{error_msg}")
            return {
                'comparison_results': {},
                'timing': {},
                'summary_file': None,
                'error': error_msg
            }
        
        # æ”¶é›†ç­‰ä»·æ€§åˆ†æçš„æ—¶é—´ä¿¡æ¯
        eq_timing = {}
        for (opt1, opt2), results in comparison_results.items():
            comparison_name = f"{opt1}_vs_{opt2}"
            if 'timing_info' in results:
                eq_timing[comparison_name] = results['timing_info']
        
        # ç¡®å®šæ‘˜è¦æ–‡ä»¶å
        if self.target_program:
            summary_file = os.path.join(analysis_dir, f"{self.target_program}_equivalence_summary.txt")
        else:
            summary_file = os.path.join(analysis_dir, "optimization_equivalence_summary.txt")
        
        return {
            'comparison_results': comparison_results,
            'timing': eq_timing,
            'summary_file': summary_file
        }
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        report_file = os.path.join(self.benchmark_dir, "integrated_analysis_report.txt")
        
        with open(report_file, "w", encoding='utf-8') as f:
            f.write("é›†æˆåˆ†æç»¼åˆæŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            
            # æ€»ä½“æ—¶é—´ç»Ÿè®¡
            f.write("æ€»ä½“æ—¶é—´ç»Ÿè®¡:\n")
            f.write("-" * 40 + "\n")
            
            total_time = self.timing_data['phase_times']['total']
            se_time = self.timing_data['phase_times']['symbolic_execution']
            eq_time = self.timing_data['phase_times']['equivalence_analysis']
            
            f.write(f"  æ€»åˆ†ææ—¶é—´: {total_time:.3f} ç§’\n\n")
            
            f.write("å„é˜¶æ®µæ—¶é—´åˆ†å¸ƒ:\n")
            f.write("-" * 40 + "\n")
            f.write(f"  ç¬¦å·æ‰§è¡Œé˜¶æ®µ: {se_time:.3f} ç§’ ({se_time/total_time*100:.1f}%)\n")
            f.write(f"  ç­‰ä»·æ€§åˆ†æé˜¶æ®µ: {eq_time:.3f} ç§’ ({eq_time/total_time*100:.1f}%)\n\n")
            
            # ç¬¦å·æ‰§è¡Œè¯¦ç»†ä¿¡æ¯
            if 'timing' in self.timing_data['symbolic_execution']:
                f.write("ç¬¦å·æ‰§è¡Œè¯¦ç»†ç»Ÿè®¡:\n")
                f.write("-" * 40 + "\n")
                
                se_timing = self.timing_data['symbolic_execution']['timing']
                if se_timing:
                    total_paths = sum(info['path_count'] for info in se_timing.values())
                    skipped_count = sum(1 for info in se_timing.values() if info.get('skipped', False))
                    actual_analyzed = len(se_timing) - skipped_count
                    
                    f.write(f"  åˆ†æçš„äºŒè¿›åˆ¶æ–‡ä»¶æ•°: {len(se_timing)}\n")
                    f.write(f"  è·³è¿‡ç¬¦å·æ‰§è¡Œçš„æ–‡ä»¶æ•°: {skipped_count}\n")
                    f.write(f"  å®é™…æ‰§è¡Œç¬¦å·æ‰§è¡Œçš„æ–‡ä»¶æ•°: {actual_analyzed}\n")
                    f.write(f"  ç”Ÿæˆçš„æ€»è·¯å¾„æ•°: {total_paths}\n")
                    
                    if len(se_timing) > 0:
                        f.write(f"  å¹³å‡æ¯ä¸ªæ–‡ä»¶è·¯å¾„æ•°: {total_paths/len(se_timing):.1f}\n")
                    
                    if actual_analyzed > 0:
                        actual_times = [info['actual_time'] for info in se_timing.values() if 'actual_time' in info and not info.get('skipped', False)]
                        if actual_times:
                            avg_time = sum(actual_times) / len(actual_times)
                            f.write(f"  å¹³å‡æ¯ä¸ªæ–‡ä»¶åˆ†ææ—¶é—´: {avg_time:.3f} ç§’\n")
                    
                    if total_paths > 0 and se_time > 0:
                        f.write(f"  å¹³å‡æ¯æ¡è·¯å¾„ç”Ÿæˆæ—¶é—´: {se_time/total_paths:.4f} ç§’\n")
                    
                    # æ£€æŸ¥è·¯å¾„è´¨é‡é—®é¢˜
                    empty_path_files = 0
                    if self.target_program:
                        # æ£€æŸ¥ç›®æ ‡ç¨‹åºçš„è·¯å¾„æ–‡ä»¶è´¨é‡
                        pattern = f"{self.target_program}_O*_path_*.txt"
                        path_files = glob.glob(pattern) + glob.glob(os.path.join(self.benchmark_dir, pattern))
                        
                        for path_file in path_files:
                            try:
                                with open(path_file, 'r') as pf:
                                    content = pf.read()
                                    if "'count': 0" in content or "çº¦æŸä¿¡æ¯: {'count': 0}" in content:
                                        empty_path_files += 1
                            except:
                                pass
                        
                        if empty_path_files > 0:
                            f.write(f"\n  âš ï¸  è·¯å¾„è´¨é‡è¯Šæ–­:\n")
                            f.write(f"     å‘ç° {empty_path_files} ä¸ªç©ºçº¦æŸè·¯å¾„æ–‡ä»¶\n")
                            f.write(f"     å¯èƒ½åŸå› : ç¨‹åºä½¿ç”¨å›ºå®šè¾“å…¥ï¼Œç¼ºå°‘ç¬¦å·åŒ–å˜é‡\n")
                            f.write(f"     å»ºè®®: æ£€æŸ¥ç¬¦å·æ‰§è¡Œé…ç½®ï¼Œç¡®ä¿æ­£ç¡®è®¾ç½®ç¬¦å·åŒ–è¾“å…¥\n")
                    
                    f.write("\n")
                else:
                    f.write("  æ²¡æœ‰è¿›è¡Œç¬¦å·æ‰§è¡Œï¼ˆä½¿ç”¨ç°æœ‰è·¯å¾„æ–‡ä»¶ï¼‰\n\n")
                
                # å„æ–‡ä»¶è¯¦ç»†ä¿¡æ¯
                f.write("å„äºŒè¿›åˆ¶æ–‡ä»¶åˆ†æè¯¦æƒ…:\n")
                for binary_name, timing_info in se_timing.items():
                    f.write(f"  {binary_name}:\n")
                    f.write(f"    è·¯å¾„æ•°: {timing_info['path_count']}\n")
                    
                    if timing_info.get('skipped', False):
                        f.write(f"    çŠ¶æ€: ä½¿ç”¨ç°æœ‰è·¯å¾„æ–‡ä»¶\n")
                    else:
                        if 'actual_time' in timing_info:
                            f.write(f"    åˆ†ææ—¶é—´: {timing_info['actual_time']:.3f} ç§’\n")
                            if timing_info['path_count'] > 0:
                                f.write(f"    æ¯è·¯å¾„æ—¶é—´: {timing_info['actual_time']/timing_info['path_count']:.4f} ç§’\n")
                        f.write(f"    çŠ¶æ€: æ–°æ‰§è¡Œå®Œæˆ\n")
                    f.write("\n")
            
            # ç­‰ä»·æ€§åˆ†æè¯¦ç»†ä¿¡æ¯
            if 'timing' in self.timing_data['equivalence_analysis']:
                eq_timing = self.timing_data['equivalence_analysis']['timing']
                
                if eq_timing:  # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´æ•°æ®
                    f.write("ç­‰ä»·æ€§åˆ†æè¯¦ç»†ç»Ÿè®¡:\n")
                    f.write("-" * 40 + "\n")
                    
                    total_comparisons = len(eq_timing)
                    total_z3_time = sum(timing.get('z3_total_time', 0) for timing in eq_timing.values())
                    total_z3_calls = sum(timing.get('z3_call_count', 0) for timing in eq_timing.values())
                    
                    f.write(f"  ä¼˜åŒ–çº§åˆ«å¯¹æ¯”è¾ƒæ•°: {total_comparisons}\n")
                    f.write(f"  Z3æ±‚è§£æ€»æ—¶é—´: {total_z3_time:.3f} ç§’ ({total_z3_time/eq_time*100:.1f}%)\n")
                    f.write(f"  Z3æ±‚è§£æ€»è°ƒç”¨æ¬¡æ•°: {total_z3_calls}\n")
                    if total_z3_calls > 0:
                        f.write(f"  å¹³å‡æ¯æ¬¡Z3æ±‚è§£æ—¶é—´: {total_z3_time/total_z3_calls:.4f} ç§’\n")
                    f.write(f"  éZ3å¤„ç†æ—¶é—´: {eq_time - total_z3_time:.3f} ç§’ ({(eq_time - total_z3_time)/eq_time*100:.1f}%)\n\n")
                    
                    # ç­‰ä»·æ€§ç»Ÿè®¡
                    total_equiv_pairs = 0
                    total_non_equiv_pairs = 0
                    
                    # ä»æ¯”è¾ƒç»“æœä¸­æå–ç­‰ä»·æ€§ä¿¡æ¯
                    if 'comparison_results' in self.timing_data['equivalence_analysis']:
                        comp_results = self.timing_data['equivalence_analysis']['comparison_results']
                        for (opt1, opt2), results in comp_results.items():
                            if 'equivalent_pairs' in results:
                                total_equiv_pairs += len(results['equivalent_pairs'])
                            if 'non_equivalent_pairs' in results:
                                total_non_equiv_pairs += len(results['non_equivalent_pairs'])
                    
                    if total_equiv_pairs + total_non_equiv_pairs > 0:
                        equiv_rate = total_equiv_pairs / (total_equiv_pairs + total_non_equiv_pairs) * 100
                        f.write(f"  ç­‰ä»·è·¯å¾„å¯¹æ€»æ•°: {total_equiv_pairs}\n")
                        f.write(f"  éç­‰ä»·è·¯å¾„å¯¹æ€»æ•°: {total_non_equiv_pairs}\n")
                        f.write(f"  ç­‰ä»·æ€§éªŒè¯ç‡: {equiv_rate:.1f}%\n\n")
                    
                    # å„æ¯”è¾ƒå¯¹è¯¦ç»†ä¿¡æ¯
                    f.write("å„ä¼˜åŒ–çº§åˆ«å¯¹æ¯”è¾ƒè¯¦æƒ…:\n")
                    for comparison_name, timing_info in eq_timing.items():
                        f.write(f"  {comparison_name}:\n")
                        f.write(f"    æ€»æ—¶é—´: {timing_info.get('total_time', 0):.3f} ç§’\n")
                        f.write(f"    æ–‡ä»¶åŠ è½½: {timing_info.get('load_time', 0):.3f} ç§’\n")
                        f.write(f"    è·¯å¾„æ¯”è¾ƒ: {timing_info.get('comparison_time', 0):.3f} ç§’\n")
                        f.write(f"    Z3æ±‚è§£: {timing_info.get('z3_total_time', 0):.3f} ç§’\n")
                        f.write(f"    Z3è°ƒç”¨: {timing_info.get('z3_call_count', 0)} æ¬¡\n")
                        f.write("\n")
                else:
                    f.write("ç­‰ä»·æ€§åˆ†æçŠ¶æ€:\n")
                    f.write("-" * 40 + "\n")
                    error_msg = self.timing_data['equivalence_analysis'].get('error', 'æœªçŸ¥é”™è¯¯')
                    f.write(f"  âŒ ç­‰ä»·æ€§åˆ†æå¤±è´¥: {error_msg}\n")
                    
                    # æä¾›è¯Šæ–­å»ºè®®
                    if self.target_program:
                        f.write(f"  ğŸ” é’ˆå¯¹ç¨‹åº {self.target_program} çš„è¯Šæ–­å»ºè®®:\n")
                        f.write(f"    1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœ‰æ•ˆçš„è·¯å¾„æ–‡ä»¶\n")
                        f.write(f"    2. éªŒè¯è·¯å¾„æ–‡ä»¶åŒ…å«æœ‰æ„ä¹‰çš„çº¦æŸ\n")
                        f.write(f"    3. ç¡®ä¿è‡³å°‘æœ‰2ä¸ªä¸åŒçš„ä¼˜åŒ–çº§åˆ«\n")
                    else:
                        f.write(f"  ğŸ” è¯Šæ–­å»ºè®®:\n")
                        f.write(f"    1. æ£€æŸ¥è·¯å¾„æ–‡ä»¶æ˜¯å¦æ­£ç¡®ç”Ÿæˆ\n")
                        f.write(f"    2. éªŒè¯æ–‡ä»¶å‘½åæ ¼å¼æ­£ç¡®\n")
                        f.write(f"    3. ç¡®ä¿ç›®å½•ä¸­æœ‰è¶³å¤Ÿçš„ä¼˜åŒ–çº§åˆ«\n")
                    f.write("\n")
            
            # æ€§èƒ½åˆ†æå’Œå»ºè®®
            f.write("æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®:\n")
            f.write("-" * 40 + "\n")
            
            if se_time > eq_time:
                f.write("  ç¬¦å·æ‰§è¡Œæ˜¯ä¸»è¦æ—¶é—´ç“¶é¢ˆ\n")
                f.write("  å»ºè®®ä¼˜åŒ–æ–¹å‘:\n")
                f.write("    - ä¼˜åŒ–çº¦æŸæ±‚è§£ç­–ç•¥\n")
                f.write("    - è°ƒæ•´è·¯å¾„æ¢ç´¢æ·±åº¦\n")
                f.write("    - å¹¶è¡ŒåŒ–ç¬¦å·æ‰§è¡Œ\n")
            else:
                f.write("  ç­‰ä»·æ€§åˆ†ææ˜¯ä¸»è¦æ—¶é—´ç“¶é¢ˆ\n")
                f.write("  å»ºè®®ä¼˜åŒ–æ–¹å‘:\n")
                f.write("    - ä¼˜åŒ–Z3æ±‚è§£å™¨é…ç½®\n")
                f.write("    - æ”¹è¿›è·¯å¾„åŒ¹é…ç®—æ³•\n")
                f.write("    - å¹¶è¡ŒåŒ–ç­‰ä»·æ€§æ£€æŸ¥\n")
            
            if 'timing' in self.timing_data['equivalence_analysis']:
                eq_timing = self.timing_data['equivalence_analysis']['timing']
                if eq_timing:
                    total_z3_time = sum(timing.get('z3_total_time', 0) for timing in eq_timing.values())
                    total_comparison_time = sum(timing.get('total_time', 0) for timing in eq_timing.values())
                    
                    if total_comparison_time > 0:
                        z3_ratio = total_z3_time / total_comparison_time
                        f.write(f"\n  Z3æ±‚è§£å ç­‰ä»·æ€§åˆ†ææ—¶é—´æ¯”ä¾‹: {z3_ratio*100:.1f}%\n")
                        if z3_ratio > 0.8:
                            f.write("    Z3æ±‚è§£æ˜¯ç­‰ä»·æ€§åˆ†æçš„ä¸»è¦ç“¶é¢ˆ\n")
                        elif z3_ratio < 0.5:
                            f.write("    è·¯å¾„åŒ¹é…å’Œé¢„å¤„ç†è€—æ—¶è¾ƒå¤š\n")
                else:
                    f.write(f"\n  âš ï¸  ç­‰ä»·æ€§åˆ†ææœªæˆåŠŸå®Œæˆ\n")
                    if self.target_program:
                        f.write(f"     è¿™å¯èƒ½æ˜¯å› ä¸ºç¨‹åº {self.target_program} çš„è·¯å¾„çº¦æŸä¸ºç©º\n")
                        f.write(f"     å»ºè®®æ£€æŸ¥ç¬¦å·æ‰§è¡Œé…ç½®æˆ–ç¨‹åºè¾“å…¥è®¾ç½®\n")
        
        print(f"\nğŸ“„ ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # æ‰“å°å…³é”®ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š å…³é”®æ€§èƒ½æŒ‡æ ‡:")
        print(f"   æ€»åˆ†ææ—¶é—´: {self.timing_data['phase_times']['total']:.3f} ç§’")
        print(f"   ç¬¦å·æ‰§è¡Œ: {self.timing_data['phase_times']['symbolic_execution']:.3f} ç§’ ({self.timing_data['phase_times']['symbolic_execution']/self.timing_data['phase_times']['total']*100:.1f}%)")
        print(f"   ç­‰ä»·æ€§åˆ†æ: {self.timing_data['phase_times']['equivalence_analysis']:.3f} ç§’ ({self.timing_data['phase_times']['equivalence_analysis']/self.timing_data['phase_times']['total']*100:.1f}%)")
        
        # æ·»åŠ ç‰¹å®šè¯Šæ–­ä¿¡æ¯
        if self.target_program:
            print(f"   ç›®æ ‡ç¨‹åº: {self.target_program}")
            # æ£€æŸ¥è·¯å¾„æ–‡ä»¶è´¨é‡
            pattern = f"{self.target_program}_O*_path_*.txt"
            path_files = glob.glob(pattern) + glob.glob(os.path.join(self.benchmark_dir, pattern))
            
            empty_constraint_files = 0
            for path_file in path_files:
                try:
                    with open(path_file, 'r') as pf:
                        content = pf.read()
                        if "'count': 0" in content:
                            empty_constraint_files += 1
                except:
                    pass
            
            if empty_constraint_files > 0:
                print(f"   âš ï¸  å‘ç° {empty_constraint_files} ä¸ªç©ºçº¦æŸè·¯å¾„æ–‡ä»¶")
                print(f"   ğŸ’¡ å»ºè®®: æ£€æŸ¥ç¨‹åºæ˜¯å¦ä½¿ç”¨ç¬¦å·åŒ–è¾“å…¥")

class QuickAnalysisMode:
    """å¿«é€Ÿåˆ†ææ¨¡å¼ - é’ˆå¯¹ç‰¹å®šç¨‹åºçš„åˆ†æ"""
    
    def __init__(self, program_name, benchmark_dir, timeout=120, force_rerun=False):
        self.program_name = program_name
        self.benchmark_dir = benchmark_dir
        self.timeout = timeout
        self.force_rerun = force_rerun
        
    def run_quick_analysis(self):
        """è¿è¡Œå¿«é€Ÿåˆ†æ - åªåˆ†ææŒ‡å®šç¨‹åºçš„æ‰€æœ‰ä¼˜åŒ–çº§åˆ«"""
        print(f"ğŸš€ å¿«é€Ÿåˆ†ææ¨¡å¼: {self.program_name}")
        print("=" * 60)
        
        # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³çš„äºŒè¿›åˆ¶æ–‡ä»¶
        binary_patterns = [f"{self.program_name}_O*"]
        
        framework = IntegratedAnalysisFramework(self.benchmark_dir, self.timeout, self.force_rerun, self.program_name)
        results = framework.run_complete_analysis(binary_patterns)
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é›†æˆçš„ç¬¦å·æ‰§è¡Œå’Œç­‰ä»·æ€§åˆ†æå·¥å…·')
    parser.add_argument('--benchmark', default='.', help='benchmarkç›®å½•è·¯å¾„')
    parser.add_argument('--timeout', type=int, default=120, help='ç¬¦å·æ‰§è¡Œè¶…æ—¶æ—¶é—´(ç§’)')
    parser.add_argument('--program', help='æŒ‡å®šç¨‹åºåè¿›è¡Œå¿«é€Ÿåˆ†æ (ä¾‹å¦‚: s000)')
    parser.add_argument('--quick', action='store_true', help='å¯ç”¨å¿«é€Ÿåˆ†ææ¨¡å¼')
    parser.add_argument('--force-rerun', '-f', action='store_true', help='å¼ºåˆ¶é‡æ–°æ‰§è¡Œç¬¦å·æ‰§è¡Œï¼Œåˆ é™¤ç°æœ‰è·¯å¾„æ–‡ä»¶')
    
    args = parser.parse_args()
    
    if args.quick and args.program:
        # å¿«é€Ÿåˆ†ææ¨¡å¼
        quick_analyzer = QuickAnalysisMode(args.program, args.benchmark, args.timeout, args.force_rerun)
        results = quick_analyzer.run_quick_analysis()
    else:
        # å®Œæ•´åˆ†ææ¨¡å¼
        framework = IntegratedAnalysisFramework(args.benchmark, args.timeout, args.force_rerun)
        results = framework.run_complete_analysis()
    
    print("\nğŸ¯ åˆ†æå®Œæˆï¼æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶è·å–è¯¦ç»†ç»“æœ:")
    print(f"   ğŸ“„ ç»¼åˆæŠ¥å‘Š: {os.path.join(args.benchmark, 'integrated_analysis_report.txt')}")
    print(f"   ğŸ“Š ç¬¦å·æ‰§è¡ŒæŠ¥å‘Š: {os.path.join(args.benchmark, 'symbolic_execution_summary.txt')}")
    print(f"   ğŸ” ç­‰ä»·æ€§åˆ†ææŠ¥å‘Š: {os.path.join(args.benchmark, 'optimization_equivalence_summary.txt')}")

if __name__ == "__main__":
    main() 